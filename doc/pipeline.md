# Experimental Pipeline: Mitigating Self-Preference

This document outlines the rigorous experimental procedure designed to validate self-preference bias in Large Language Models (LLMs) and test the efficacy of authorship obfuscation.

## Phase 1: Data Preparation

### 1. Dataset Selection & Generation

* **Source**: QuALITY dataset (Long-context Question Answering).
* **Models**: 5 models of varying capabilities (e.g., Llama-3.1, Qwen2.5, DeepSeek-V3).
* **Action**: Generate answers for the validation set using these models.

### 2. Adversarial Sample Construction (Filtering Step 1)

* **Pairing**: Construct answer pairs from all model responses.
* **Tie-Breaking**:
  * Remove pairs where both models are correct.
  * Remove pairs where both models are incorrect.
  * **Keep**: Only pairs where strictly **one is correct and one is wrong**.
* **Goal**: Ensure an objective ground-truth difference exists to measure judgment accuracy.

## Phase 2: Baseline Evaluation

### 1. Judge Evaluation & Denoising (Filtering Step 2)

The model acts as a judge to evaluate the remaining pairs.

* **Position Bias Control**: Evaluate every pair twice by swapping orders (A vs B, then B vs A).
* **Ambiguous Decision Removal**:
  * If the judge behaves inconsistently (e.g., favors A in position 1 but B in position 1 when swapped), the sample is "Ambiguous".
  * **Action**: Discard these samples.
  * **Goal**: Ensure retained samples represent stable, "thoughtful" decisions.

### 2. Metrics Calculation

Statistics are calculated on the cleaned dataset:

1. **All Examples**: Overall judgment accuracy.
2. **Self-evaluation**: Accuracy when the judge evaluates "Self vs Other".
3. **Harmful Cases (Core Metric)**: Accuracy when the **Judge is Wrong** and the **Opponent is Correct**.
    * *Significance*: Low accuracy here indicates the model prefers its own wrong answer over a correct opposing answer (Self-Preference).

## Phase 3: Intervention (Authorship Obfuscation)

### Synonym Replacement

* **Target**: Only modify the answer generated by the judge itself.
* **Operation**:
    1. Identify non-key words (excluding stop words and question-specific terms).
    2. Use an external model (e.g., Llama-3.3-70B) to replace **2** of these words with synonyms.
* **Goal**: Disrupt stylistic cues that allow "Self-Recognition" without altering the semantic meaning or correctness of the answer.

## Phase 4: Comparative Analysis

### Re-evaluation

The judge re-evaluates the modified pairs ("Obfuscated Self" vs "Opponent").

### Analysis Logic

* **Compare Core Metrics**: Check if accuracy in **Harmful Cases** improves.
* **Inference**:
  * If accuracy rises (i.e., the model switches to the correct opponent) after merely changing two synonyms in its own wrong answer.
  * **Conclusion**: The initial refusal to switch was driven by recognizing its own writing style (Self-Preference). The intervention successfully mitigated this bias.

---

### Key Validity Factors
>
> **1. Strict Content Logic**
> Filtering for "One Correct, One Wrong" is the prerequisite for the experiment. Without this, "favoritism" vs "correctness" cannot be distinguished.
>
> **2. Robustness via Permutation**
> Swapping orders minimizes position bias and acts as a noise filter to remove random guessing.
